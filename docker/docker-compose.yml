# Gemeinsames internes Netzwerk für alle Services
networks:
  rag-net:
    driver: bridge

services:
  # ============================================================================
  # Frontend - VueJS Chat & Admin Interface
  # ============================================================================
  frontend:
    build:
      context: ..
      dockerfile: docker/frontend.Dockerfile
    container_name: rag-frontend
    ports:
      - "5173:80" # Nginx Production Server (extern erreichbar)
    environment:
      - VITE_API_URL=http://localhost:8080
    networks:
      - rag-net
    depends_on:
      - backend-orchestrator
    restart: unless-stopped

  # ============================================================================
  # Backend Orchestrator - Rust Backend für RAG-Pipeline
  # ============================================================================
  backend-orchestrator:
    build:
      context: ../apps/backend-orchestrator
      dockerfile: ../../docker/backend.Dockerfile
    container_name: rag-backend-orchestrator
    ports:
      - "8080:8080" # API Endpoint (extern erreichbar)
    environment:
      - RUST_LOG=info
      - MEILISEARCH_URL=http://meilisearch:7700
      - MEILISEARCH_API_KEY=${MEILISEARCH_API_KEY}
      - EMBEDDING_API_URL=http://embedding-api:8080
      - LLM_API_URL=http://llm-inference:8080
      - SERVER_PORT=8080
    networks:
      - rag-net
    depends_on:
      meilisearch:
        condition: service_healthy
      embedding-api:
        condition: service_healthy
      # llm-inference:
      #   condition: service_healthy
    restart: unless-stopped
    # Ressourcenlimits für Backend
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 512M

  # ============================================================================
  # Meilisearch - Vektor- und Metadaten-Speicher
  # ============================================================================
  meilisearch:
    image: getmeili/meilisearch
    container_name: rag-meilisearch
    ports:
      - 7700:7700
    # KEIN externer Port - nur intern erreichbar
    expose:
      - "7700"
    environment:
      - MEILI_ENV=development
      - MEILI_MASTER_KEY=${MEILISEARCH_API_KEY}
      - MEILI_NO_ANALYTICS=true
    volumes:
      - meilisearch-data:/meili_data
    networks:
      - rag-net
    restart: unless-stopped
    # Ressourcenlimits für Meilisearch
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 4G
        reservations:
          cpus: "0.5"
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7700/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # ============================================================================
  # Embedding API - Text Embeddings Inference (TEI)
  # ============================================================================
  embedding-api:
    image: ollama/ollama
    container_name: rag-embedding-api
    volumes:
      - embedding_ollama_data:/root/.ollama
      - ../services/embedding-api/entrypoint.sh:/entrypoint.sh
    environment:
      - OLLAMA_HOST=0.0.0.0
    networks:
      - rag-net
    restart: unless-stopped
    # Ressourcenlimits für Embedding Service
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 4G
        reservations:
          cpus: "1.0"
          memory: 2G
    healthcheck:
      test: "ollama --version && ollama ps || exit 1"
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    # Optional: GPU Support aktivieren
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # ============================================================================
  # LLM Inference - llama.cpp Server für Chat Completions
  # ============================================================================
  # llm-inference:
  #   image: ollama/ollama
  #   container_name: rag-llm-inference
  #   environment:
  #     - OLLAMA_HOST=0.0.0.0
  #   # KEIN externer Port - nur intern erreichbar
  #   expose:
  #     - "8080"
  #   volumes:
  #     - llm_ollama_data:/root/.ollama
  #     - ../services/llm-inference/entrypoint.sh:/entrypoint.sh
  #   networks:
  #     - rag-net
  #   restart: unless-stopped
  #   entrypoint: ["/entrypoint.sh"]
  #   # Ressourcenlimits für LLM Inference
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "4.0"
  #         memory: 8G
  #       reservations:
  #         cpus: "2.0"
  #         memory: 4G
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 120s
  # Optional: GPU Support aktivieren
  # deploy:
  #   resources:
  #     reservations:
  #       devices:
  #         - driver: nvidia
  #           count: 1
  #           capabilities: [gpu]
# ============================================================================
# Hinweise zur Verwendung:
# ============================================================================
#
# 1. Umgebungsvariablen:
#    Kopiere docker/.env.example zu docker/.env und passe die Werte an:
#
#    cp docker/.env.example docker/.env
#    nano docker/.env
#
# 2. LLM Modell herunterladen:
#    Vor dem ersten Start muss ein GGUF-Modell heruntergeladen werden:
#
#    docker run -v llm-models:/models alpine sh -c \
#      "wget https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_M.gguf -O /models/model.gguf"
#
# 3. Services starten:
#    docker-compose -f docker/docker-compose.yml up -d
#
# 4. Logs anzeigen:
#    docker-compose -f docker/docker-compose.yml logs -f
#
# 5. Services stoppen:
#    docker-compose -f docker/docker-compose.yml down
#
# 6. Volumes löschen (Vorsicht - löscht alle Daten):
#    docker-compose -f docker/docker-compose.yml down -v
#
# ============================================================================

# Persistent Volumes für Daten
volumes:
  meilisearch-data:
  embedding_ollama_data:
  llm_ollama_data:
