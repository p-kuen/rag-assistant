# RAG Assistant - Ressourcenoptimierte Fullstack RAG-Plattform

Ein modulares, Docker-basiertes RAG (Retrieval-Augmented Generation) System mit VueJS Frontend und Rust Backend.

[![Architecture](https://img.shields.io/badge/Architecture-Microservices-blue)](./docs/ARCHITECTURE.md)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?logo=docker)](./docker/docker-compose.yml)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](./LICENSE)

## ğŸš€ Features

- âœ… **Chat-Assistent**: Interaktive RAG-basierte Konversation mit Quellenangaben
- âœ… **Wissensverwaltung**: Dokument-Upload und manuelle Markdown-Eingabe
- âœ… **Hybrid Search**: Kombination aus Vektor- und Keyword-Suche via Meilisearch
- âœ… **Streaming Responses**: Real-time Token-Streaming vom LLM
- âœ… **Ressourcenoptimiert**: Kompakte Modelle und effiziente Container-Orchestrierung
- âœ… **OpenAI-kompatibel**: Standard-APIs fÃ¼r Embedding und LLM
- âœ… **VollstÃ¤ndige RAG-Pipeline**: Parsing, Chunking, Embedding, Indexierung, Retrieval, Generation

## ğŸ“ Monorepo-Struktur

```
rag-assistant/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ frontend/              # VueJS 3 + TypeScript + Vite
â”‚   â””â”€â”€ backend-orchestrator/  # Rust (Axum) Backend
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ meilisearch/          # Vektor- & Metadaten-Speicher
â”‚   â”œâ”€â”€ embedding-api/        # Text Embeddings (TEI)
â”‚   â””â”€â”€ llm-inference/        # LLM Service (llama.cpp)
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yml    # Orchestrierung aller Services
â”‚   â””â”€â”€ *.Dockerfile
â””â”€â”€ docs/                      # Umfassende Dokumentation
```

## ğŸ—ï¸ Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend   â”‚ :5173 (VueJS)
â”‚  (Vue 3)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ REST API
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Backend   â”‚ :8080 (Rust)
â”‚ Orchestratorâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â–¼   â–¼        â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Meiliâ”‚â”‚Embed â”‚â”‚  LLM   â”‚
â”‚srch â”‚â”‚ API  â”‚â”‚Inferencâ”‚
â””â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Siehe [ARCHITECTURE.md](./docs/ARCHITECTURE.md) fÃ¼r Details.

## ğŸš€ Quick Start

### Voraussetzungen

- **Node.js** >= 18.0.0
- **Pnpm** >= 8.0.0
- **Docker** und **Docker Compose**
- **Rust** >= 1.75 (optional, fÃ¼r Backend-Entwicklung)

### 1. Installation

```bash
# Repository klonen
git clone <repository-url>
cd rag-assistant

# Dependencies installieren
pnpm install
```

### 2. LLM Modell herunterladen

```bash
# Gemma-2-2B quantisiert (~1.5GB)
docker run -v rag-assistant_llm-models:/models alpine sh -c \
  "apk add --no-cache wget && \
   wget https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_M.gguf \
   -O /models/model.gguf"
```

### 3. Environment konfigurieren

```bash
# .env Datei erstellen
cp docker/.env.example docker/.env

# API Keys setzen
nano docker/.env
```

### 4. Services starten

```bash
# Alle Docker Services starten
pnpm docker:up

# Logs verfolgen
pnpm docker:logs
```

### 5. Development

```bash
# Frontend (http://localhost:5173)
pnpm dev:frontend

# Backend (http://localhost:8080)
pnpm dev:backend
```

## ğŸ“¦ VerfÃ¼gbare Scripts

```bash
# Development
pnpm dev              # Alle Apps im Dev-Modus
pnpm dev:frontend     # Nur Frontend
pnpm dev:backend      # Nur Backend

# Build
pnpm build            # Alle Apps bauen
pnpm build:frontend   # Frontend Production Build
pnpm build:backend    # Backend Release Build

# Docker
pnpm docker:up        # Services starten
pnpm docker:down      # Services stoppen
pnpm docker:logs      # Logs anzeigen
```

## ğŸ› ï¸ Technologie-Stack

### Frontend
- **Vue 3** - Progressive JavaScript Framework
- **TypeScript** - Type Safety
- **Vite** - Build Tool & Dev Server
- **Tailwind CSS v4** - Styling
- **Vue Router** - Routing

### Backend
- **Rust** - Systems Programming Language
- **Axum** - Async Web Framework
- **Tokio** - Async Runtime
- **Reqwest** - HTTP Client

### Services
- **Meilisearch** - Hybrid Search Engine
- **HuggingFace TEI** - Embedding API
- **llama.cpp** - LLM Inference

## ğŸ“š Dokumentation

- **[Hauptdokumentation](./docs/README.md)** - Umfassende Ãœbersicht
- **[Architektur](./docs/ARCHITECTURE.md)** - System-Design & RAG-Pipeline
- **[Deployment](./docs/DEPLOYMENT.md)** - Deployment-Anleitungen & Tuning

## ğŸ¯ Use Cases

- **Interne Wissensdatenbank**: Unternehmensdokumentation durchsuchbar machen
- **Customer Support**: FAQ-basierte Assistenzsysteme
- **Research Assistant**: Wissenschaftliche Paper-Analyse
- **Code Documentation**: Code-Repository-Abfragen

## ğŸ”’ Sicherheit

- âœ… Interne Services sind nicht extern erreichbar
- âœ… Meilisearch API Key erforderlich
- âœ… Container laufen mit non-root User
- âœ… Docker Network Isolation

## ğŸ“Š Ressourcenanforderungen

| Service | Min. RAM | Min. CPU | Empfohlen |
|---------|----------|----------|-----------|
| Frontend | 512MB | 0.5 | 1GB / 1 CPU |
| Backend | 512MB | 0.5 | 2GB / 2 CPU |
| Meilisearch | 1GB | 0.5 | 4GB / 2 CPU |
| Embedding API | 2GB | 1.0 | 4GB / 2 CPU |
| LLM Inference | 4GB | 2.0 | 8GB / 4 CPU |

**Total:** Min. 8GB RAM, 4 CPU Cores

## ğŸ”§ Troubleshooting

### Services starten nicht
```bash
# Status checken
docker-compose -f docker/docker-compose.yml ps

# Logs fÃ¼r spezifischen Service
docker-compose -f docker/docker-compose.yml logs llm-inference

# Service neu starten
docker-compose -f docker/docker-compose.yml restart llm-inference
```

### "Model not found" Error
Das LLM-Modell wurde nicht heruntergeladen. Siehe Quick Start Schritt 2.

### Port bereits belegt
Andere Anwendung nutzt Port 5173 oder 8080:
```bash
# Ports in docker-compose.yml Ã¤ndern:
ports:
  - "5174:5173"  # Frontend auf 5174
  - "8081:8080"  # Backend auf 8081
```

### Out of Memory
Docker Memory-Limit erhÃ¶hen:
- **Docker Desktop:** Settings â†’ Resources â†’ Memory â†’ Min. 8GB

### Frontend zeigt Fehler
1. Backend lÃ¤uft nicht â†’ `pnpm dev:backend` oder Docker-Services starten
2. CORS-Error â†’ Backend API URL in `.env` prÃ¼fen

### Meilisearch Index nicht initialisiert
```bash
# Manuell initialisieren
docker exec rag-meilisearch /init_meilisearch.sh
```

## ğŸ¤ Beitragen

1. Fork das Repository
2. Feature Branch erstellen (`git checkout -b feature/amazing-feature`)
3. Ã„nderungen committen (`git commit -m '[frontend] Add amazing feature'`)
4. Branch pushen (`git push origin feature/amazing-feature`)
5. Pull Request Ã¶ffnen

## ğŸ“ Lizenz

[Ihre Lizenz hier - z.B. MIT]

## ğŸ™ Acknowledgments

- **Meilisearch** - Hybrid Search Engine
- **HuggingFace** - Text Embeddings Inference
- **llama.cpp** - Efficient LLM Inference
- **Vue.js Team** - Frontend Framework

---

**Built with â¤ï¸ using Rust, Vue, and Open Source AI**
